{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:47:48.056848Z",
     "iopub.status.busy": "2025-04-06T07:47:48.056661Z",
     "iopub.status.idle": "2025-04-06T07:48:40.363111Z",
     "shell.execute_reply": "2025-04-06T07:48:40.362143Z",
     "shell.execute_reply.started": "2025-04-06T07:47:48.056829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:48:42.649618Z",
     "iopub.status.busy": "2025-04-06T07:48:42.649267Z",
     "iopub.status.idle": "2025-04-06T07:49:04.417285Z",
     "shell.execute_reply": "2025-04-06T07:49:04.416293Z",
     "shell.execute_reply.started": "2025-04-06T07:48:42.649586Z"
    },
    "id": "VLzgZ14X_rMs",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "from trl import SFTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:49:04.419827Z",
     "iopub.status.busy": "2025-04-06T07:49:04.418706Z",
     "iopub.status.idle": "2025-04-06T07:49:04.948201Z",
     "shell.execute_reply": "2025-04-06T07:49:04.947441Z",
     "shell.execute_reply.started": "2025-04-06T07:49:04.419784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:49:04.950249Z",
     "iopub.status.busy": "2025-04-06T07:49:04.949931Z",
     "iopub.status.idle": "2025-04-06T07:49:18.365772Z",
     "shell.execute_reply": "2025-04-06T07:49:18.365103Z",
     "shell.execute_reply.started": "2025-04-06T07:49:04.950219Z"
    },
    "id": "na9CAoHC5gM9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wb_token = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune Llama 3 8B on Medical Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:50:20.593109Z",
     "iopub.status.busy": "2025-04-06T07:50:20.592789Z",
     "iopub.status.idle": "2025-04-06T07:50:20.597578Z",
     "shell.execute_reply": "2025-04-06T07:50:20.596795Z",
     "shell.execute_reply.started": "2025-04-06T07:50:20.593087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "base_model =  \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "dataset_name = \"ruslanmv/ai-medical-chatbot\"\n",
    "new_model = \"tinyLlama-medical-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:50:30.699803Z",
     "iopub.status.busy": "2025-04-06T07:50:30.699470Z",
     "iopub.status.idle": "2025-04-06T07:50:30.705033Z",
     "shell.execute_reply": "2025-04-06T07:50:30.704232Z",
     "shell.execute_reply.started": "2025-04-06T07:50:30.699777Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:50:54.989169Z",
     "iopub.status.busy": "2025-04-06T07:50:54.988820Z",
     "iopub.status.idle": "2025-04-06T07:50:54.993530Z",
     "shell.execute_reply": "2025-04-06T07:50:54.992723Z",
     "shell.execute_reply.started": "2025-04-06T07:50:54.989142Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model =  \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T07:50:56.302911Z",
     "iopub.status.busy": "2025-04-06T07:50:56.302617Z",
     "iopub.status.idle": "2025-04-06T07:50:56.572089Z",
     "shell.execute_reply": "2025-04-06T07:50:56.570823Z",
     "shell.execute_reply.started": "2025-04-06T07:50:56.302888Z"
    },
    "id": "StJKGiDDHzdk",
    "outputId": "871214ba-6c30-4ecf-ac68-550f296b7ef6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T07:49:19.588560Z",
     "iopub.status.idle": "2025-04-06T07:49:19.588823Z",
     "shell.execute_reply": "2025-04-06T07:49:19.588717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T07:49:19.589381Z",
     "iopub.status.idle": "2025-04-06T07:49:19.589767Z",
     "shell.execute_reply": "2025-04-06T07:49:19.589612Z"
    },
    "id": "XzF2UjPvTBag",
    "outputId": "3733e45f-605e-4564-88c7-368c9c5bf9cd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Importing the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
    "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= 4,\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T07:49:19.590635Z",
     "iopub.status.idle": "2025-04-06T07:49:19.590995Z",
     "shell.execute_reply": "2025-04-06T07:49:19.590836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset['text'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T07:49:19.591892Z",
     "iopub.status.idle": "2025-04-06T07:49:19.592252Z",
     "shell.execute_reply": "2025-04-06T07:49:19.592092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T07:49:19.593441Z",
     "iopub.status.idle": "2025-04-06T07:49:19.593813Z",
     "shell.execute_reply": "2025-04-06T07:49:19.593656Z"
    },
    "id": "peOnLAAhS0y1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-06T07:49:19.594603Z",
     "iopub.status.idle": "2025-04-06T07:49:19.594963Z",
     "shell.execute_reply": "2025-04-06T07:49:19.594806Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Hyperparamter\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim= \"adamw_8bit\",#\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,  # Changed from 0.2 to 100\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,      # Changed from False to True\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",\n",
    "    save_total_limit=2,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:41:20.085519Z",
     "iopub.status.busy": "2025-03-29T10:41:20.085200Z",
     "iopub.status.idle": "2025-03-29T10:41:21.370257Z",
     "shell.execute_reply": "2025-03-29T10:41:21.369544Z",
     "shell.execute_reply.started": "2025-03-29T10:41:20.085467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:41:21.371325Z",
     "iopub.status.busy": "2025-03-29T10:41:21.371127Z",
     "iopub.status.idle": "2025-03-29T10:41:21.375691Z",
     "shell.execute_reply": "2025-03-29T10:41:21.374853Z",
     "shell.execute_reply.started": "2025-03-29T10:41:21.371308Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set environment variable to help with memory fragmentation\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:46:43.776296Z",
     "iopub.status.busy": "2025-03-29T10:46:43.775981Z",
     "iopub.status.idle": "2025-03-29T10:46:56.946777Z",
     "shell.execute_reply": "2025-03-29T10:46:56.945345Z",
     "shell.execute_reply.started": "2025-03-29T10:46:43.776273Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-29T10:41:23.663143Z",
     "iopub.status.busy": "2025-03-29T10:41:23.662848Z",
     "iopub.status.idle": "2025-03-29T10:41:36.681265Z",
     "shell.execute_reply": "2025-03-29T10:41:36.680136Z",
     "shell.execute_reply.started": "2025-03-29T10:41:23.663121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKgZBEGVS5a2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L7vHP41ITQPb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I have bad acne. How do I get rid of it?\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Hello doctor, I always feel weak, can you help me with that?\"}]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
